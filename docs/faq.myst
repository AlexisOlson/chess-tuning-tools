# Frequently Asked Questions

## Setup & Settings

### How large should the ranges be?
Ideally, the optimum is exactly in the center of the range and the playing
strength of the engine gets worse in all directions.
Obviously we do not know the optimum in advance. Thus, we usually want to err
on the side of slightly too large ranges.
```{glossary}
Example 1
    We have one parameter and suspect it to lie in between 1 and 2, but
    are not 100% certain. We set the parameter range to `Real(0.0, 3.0)` to
    avoid missing the optimum. A range of `Real(0.0, 10.0)` or even
    `Real(0.0, 100.0)` would have been overkill and likely hurt the convergence.
Example 2
    We have a parameter which lies in the range [0, 1], but are not even sure
    what the correct order of magnitude is (e.g. could be 0.3 or even 0.001).
    Here it is useful to specify a range like
    `Real(1e-4, 1, prior=log-uniform)`.
    Without the prior, it will be almost impossible or the tuner to find an
    optimum close to 0.
```
```{note}
You can increase the ranges in your tuning configuration later in the process.
You only need to restart the tuner and it will start exploring the new regions.
Decreasing the ranges is possible as well, but potentially results in a loss of
data (i.e. all data points outside of the new ranges are discarded).
```

### Should I decrease the ranges late in the tune to "zoom in"?
In general you should be very careful with zooming in to the optimum.
Even though it can appear that nothing bad should happen when removing bad
regions, it can actually destabilize the model for quite a few iterations.
The reason is that bad regions have a lot of leverage and help inform the model
about the curvature of the space and the location of the optimum.

When *should* you consider zooming in? If the optimum is in a very small region
of the space (below 10%) and the rest of the landscape is either flat or of
constant slope, it can help to zoom into the relevant region.
Make sure that you do not remove the bad regions around the optimum when doing
so.

## Plots
```{figure} _static/plot_example.png
---
height: 400px
name: example-plot
---
Example optimization landscape for a two parameter tune.
```
### What is the partial dependence shown in the plots?
First of all, the plots show the negative Elo score (the optimizer expects a
function to minimize) divided by 100. Thus, lower values correspond to stronger
regions of the space. Now often we want to visualize a complicated optimization
landscape with more than 2 parameters. This is where we use the concept of
*partial dependence*. A partial dependence plot shows the marginal effect a
set of parameters has on playing strength, if we were to average out the other
parameters.

Take a look at {numref}`example-plot`. On the diagonal we have the
one-dimensional partial dependence for each of the two parameters.
The off-diagonal shows the joint effect of the parameters on the playing
strength. As is apparent, the global optimum (shown in red here) does not have
to correspond to the minima of the marginals.

If you want to know more about partial dependence, take a look at
Chapter
[5.1 Partial Dependence Plot (PDP)](https://christophm.github.io/interpretable-ml-book/pdp.html)
of the book
[Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/)
by Christoph Molnar.

### What are the red/orange dots and lines?
The red dots/lines show the position of the current best guess of the global
optimum. During the optimization process the global optimum can often jump
around as soon as a particular region looks very promising. For this reason,
we also plot a “pessimistic” optimum (orange), which takes into account the
uncertainty still present in the prediction. It is defined as the following
minimum:

$$
\DeclareMathOperator*{\argmin}{arg\,min}
\argmin_{x \in \mathcal{X}} \bigl\{ \mu(x) + 1.96 \cdot \sigma(x) \bigr\}
$$

Where $\mu(\cdot)$ is our model function for the expected playing strength and
$\sigma(\cdot)$ the uncertainty of the model in that estimate.
The 1.96 was chosen to have approximately 95% of the possible values contained
in the uncertainty region.

What could you use these for? One possibility is to use these as a necessary
condition for convergence. If the true global optimum has been confidently
found, both optima have to be on the same point. Thus if they are still widely
different, you know that you have to continue running the tuning process.

### The optimization landscape is very chaotic, with many local minima. What can I do against that?
This can happen in rare cases when the noise is low and the model drifts into
a region of the model parameter space where it is overfitting the data.
If simply waiting for a few iterations does not resolve the situation,
you can try restarting the tuner and letting it reinitialize. This usually fixes
the problem.

### Why is the partial dependence in the plot spanning only a small Elo range?
The two major causes for this are (1) the parameters you are tuning only have
a weak effect on playing strength or (2) you did not collect enough points yet
and the model still thinks it can explain the observations by a flat landscape
and noise.
With (1) you could try increasing the ranges of the parameters such that they
include obviously bad values. Other than that, there is nothing really you can
do, other than gaining the insight that the parameters are not that important.
One important reason could be that you have a bug in your implementation and
the parameter(s) in question really do have no effect.
(2) usually only happens if the sample size is still low and the signal-to-noise
ratio is low. Then you just need to wait for more samples.
Sometimes a restart of the tuner can help, since it will reinitialize with
more samples and thus can adapt more quickly.


